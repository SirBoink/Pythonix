My research approach was centered on a rigorous, pairwise evaluation of two promising, lightweight open-source models Qwen-2.5-Coder-Instruct and CodeLlama-7B-Instruct-hf from Hugging face to determine their suitability for student competence analysis. I chose Qwen-2.5-Coder and CodeLlama-7B because they are open-source, built for coding, and most importantly, can run on Google Colab, making this research fully reproducible without any cost. Also as they are  specifically trained on code and fine-tuned for instruction-following. To properly challenge them with real-word scenarios, I built a custom dataset of 10 Python problems sourced from real-world bug repositories like QuixBugs and Refactory and even included my own code from Helsinki Python MOOC 2025 tests. Also, the code selection was not random, it was carefully selected to cover five basic programming paradigms for python learner from beginner to advanced level, ensuring a realistic and comprehensive evaluation of their analytical abilities. 

To validate their performance, I used a scientific, "LLM as a Judge" framework where outputs were assessed against a ground-truth analysis using a research-backed metric based on CodeLKT and the Zone of Proximal Development. The key to testing their teaching ability, not just their bug-finding skill, was building a custom "Pythonix" persona prompt (a system instructions prompt) that forced them to ask guiding questions instead of just handing out the answers, while providing a meaningful template based on state of the art NLP frameworks. This decision-making process allowed me to measure their practical value, directly comparing their potential for being student competence analysts against their limitations (like frequent errors and bug hallucinations) within the constraints of building free and accessible educational tools with open source generative AI.
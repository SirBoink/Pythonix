Evaluation Rubric for "LLM as a Judge": Assessing Student Competence Analysis in Python

This rubric is designed for a pairwise comparison of two large language models (LLMs), referred to as Model A and Model B. The 'LLM as a judge' will evaluate the outputs of these two models on their ability to analyze Python code written by a student and provide feedback that assesses competence. The evaluation should be guided by the principles of state-of-the-art 'LLM as a judge' frameworks, which emphasize structured, criteria-based assessment to ensure reliability and mitigate bias.

The judge LLM will be provided with the student's Python code and the corresponding outputs from Model A and Model B. For each of the 10 student codes, the judge will perform a comparative evaluation using the following criteria.
I. Overall Assessment

The judge will make a final determination of which model's output is superior.

    Overall Winner: (Model A / Model B / Tie)

    Confidence Score: (1-5, where 5 is very confident)

    Justification: A detailed explanation for the overall decision, summarizing the key strengths and weaknesses of each model based on the dimensional analysis below.

II. Dimensional Evaluation

For each dimension, the judge will rate the models on a comparative scale and provide a rationale supported by evidence from the outputs. The evaluation prompts should instruct the judge to assess each output based on the following criteria.

1. Code Analysis & Conceptual Understanding

This dimension evaluates the model's ability to analyze the student's Python code beyond surface-level syntax, focusing on the underlying logic and conceptual application.

    Comparative Rating:

        Model A is significantly better

        Model A is slightly better

        Models are comparable

        Model B is slightly better

        Model B is significantly better

    Evaluation Checklist:

        Does the model correctly identify the purpose and functionality of the student's code?

        Does the analysis go beyond syntax to comment on algorithmic efficiency, code structure, and adherence to Pythonic principles?

        Does the model recognize both correct and incorrect applications of Python concepts?

    Rationale: Provide a brief justification for the rating, with examples.

2. Quality of Generated Prompts for Assessment

This dimension assesses the effectiveness of the prompts generated by the models to test the student's conceptual understanding.

    Comparative Rating:

        Model A is significantly better

        Model A is slightly better

        Models are comparable

        Model B is slightly better

        Model B is significantly better

    Evaluation Checklist:

        Are the generated prompts open-ended and thought-provoking?

        Do the prompts target specific Python concepts relevant to the student's code?

        Do the prompts effectively probe for deeper understanding rather than rote memorization?

        Are the prompts of appropriate difficulty for the inferred skill level of the student?

    Rationale: Provide a brief justification for the rating, with examples.

3. Identification of Gaps and Misconceptions

This dimension evaluates the model's accuracy in identifying and explaining a student's misconceptions or gaps in reasoning.

    Comparative Rating:

        Model A is significantly better

        Model A is slightly better

        Models are comparable

        Model B is slightly better

        Model B is significantly better

    Evaluation Checklist:

        Does the model accurately pinpoint specific errors in logic or understanding?

        Does the model explain why something is a misconception in a clear and constructive way?

        Does the model avoid being overly critical or discouraging?

        Does the model distinguish between minor syntax errors and fundamental conceptual misunderstandings?

    Rationale: Provide a brief justification for the rating, with examples.

4. Pedagogical Value and Guidance

This dimension assesses the model's ability to guide the student toward deeper learning without simply providing the correct answer. This aligns with pedagogical best practices where the goal is to foster independent problem-solving.

    Comparative Rating:

        Model A is significantly better

        Model A is slightly better

        Models are comparable

        Model B is slightly better

        Model B is significantly better

    Evaluation Checklist:

        Does the feedback encourage the student to think critically and reflect on their code?

        Does the model provide hints or ask leading questions instead of giving away the solution?

        Is the guidance actionable and relevant to the student's specific submission?

        Does the model suggest relevant concepts or documentation for the student to explore?

    Rationale: Provide a brief justification for the rating, with examples.

5. Holistic Qualities

This dimension evaluates the overall quality of the model's output in terms of communication and presentation.

    Comparative Rating:

        Model A is significantly better

        Model A is slightly better

        Models are comparable

        Model B is slightly better

        Model B is significantly better

    Evaluation Checklist:

        Clarity and Conciseness: Is the feedback easy to understand and free of jargon?

        Tone: Is the tone encouraging, respectful, and appropriate for an educational context?

        Structure: Is the output well-organized and easy to follow?

    Rationale: Provide a brief justification for the rating, with examples.

Best Practices for the "LLM as a Judge"

To ensure a fair and robust evaluation, the "LLM as a judge" should adhere to the following best practices inspired by SOTA frameworks:

    Mitigation of Positional Bias: To counteract the tendency to favor the first or second response, the positions of Model A's and Model B's outputs should be randomly swapped across the 10 evaluation runs for each student's code. The final score would be an average of these runs.

Reference-Free Evaluation: The judge should base its evaluation solely on the provided rubric and the outputs of the two models, without a "gold standard" answer, which is suitable for open-ended educational tasks.

Chain-of-Thought Reasoning: The judge should generate a step-by-step rationale for its decisions on each dimension, which enhances transparency and allows for a meta-evaluation of the judge's reasoning process.

Use of Checklists: The checklist-based approach, inspired by frameworks like CheckEval, promotes a more decomposed and reliable evaluation compared to simple Likert scales